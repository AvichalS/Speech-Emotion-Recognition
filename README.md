# Speech-Emotion-Recognition-System
SER comprises audio techniques and deep learning methods in an act to recognize human emotion from speech. This project aims to build a deep learning model for recognizing emotions from speech signals. The model is based on Multi-Layer Perceptron (MLP) architecture and is trained on a dataset of speech samples with labeled emotions.

The project follows the following steps:

1. Audio Feature Extraction: The raw audio files are preprocessed to extract meaningful features such as Mel-Frequency Cepstral Coefficients (MFCCs), which are commonly used for speech signal processing.
2. Hyperparameter Tuning: The model is trained with various hyperparameters and the best set of hyperparameters is selected using cross-validation.
3. Deep Learning using MLP: The model architecture is an MLP neural network with multiple hidden layers. The network is trained using the selected hyperparameters.
4. Testing on Sample Files: The trained model is tested on sample speech files to evaluate its performance.
5. Building a Dashboard: Finally, a dashboard is built to allow users to record audio and classify emotions using the trained model.
